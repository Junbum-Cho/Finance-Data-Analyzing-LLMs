{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPD+m0cZJ04x83yj7y6f8Kp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"Yj8hIw2y3ZPV","executionInfo":{"status":"ok","timestamp":1756440826352,"user_tz":-540,"elapsed":17112,"user":{"displayName":"BumBum Cho","userId":"04453444281969177716"}},"outputId":"7efcf8f1-05b3-4d11-f852-2ab108b43f33"},"outputs":[{"output_type":"stream","name":"stdout","text":["Upload ONE merged CSV (e.g., Merged_*_Last_24_months.csv)…\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-c6ade00d-381f-4b78-bebe-08472695ca91\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-c6ade00d-381f-4b78-bebe-08472695ca91\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Merged_Subprime_Bubble.csv to Merged_Subprime_Bubble.csv\n","\n","Indicators detected: ['CPI', 'PPI', 'FEDFUNDS', 'DGS10', 'SP500_PE', 'DJIA']\n","Missing months (1..24) by indicator: {'DGS10': [18]}\n","Created JSON (sanitized): anon_4612e7e0_window_llm_noimpute.json\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_0e3bb942-4f60-4832-81d3-17b187ea13bd\", \"anon_4612e7e0_window_llm_noimpute.json\", 2247)"]},"metadata":{}}],"source":["# -*- coding: utf-8 -*- for 24 MONTHS\n","# Colab: Upload ONE merged CSV (24+ rows) → produce ONE SANITIZED LLM-ready YAML (no imputation) and download it.\n","# Sanitization: no original filename, no calendar dates/timeline in the YAML.\n","\n","import io, os, re, math, json, uuid\n","import numpy as np\n","import pandas as pd\n","\n","# ---- Colab upload helper\n","try:\n","    from google.colab import files as _files\n","except Exception as e:\n","    raise RuntimeError(\"This cell is intended to run in Google Colab.\") from e\n","\n","# =========================\n","# Config\n","# =========================\n","WINDOW_LEN   = 24\n","TAKE_LAST_24 = True     # True: use last 24 rows after sorting by date; False: first 24\n","# If denominator is zero in percent-change, we drop that return (no error, no imputation).\n","\n","# Canonical indicator keys and flexible aliases (case-insensitive)\n","INDICATOR_ALIASES = {\n","    \"CPI\":       [r\"^cpi$\"],\n","    \"PPI\":       [r\"^ppi$\"],\n","    \"FEDFUNDS\":  [r\"^fedfunds$|^federal[_\\s]?funds|^ffr$|^policy[_\\s]?rate$\"],\n","    \"DGS10\":     [r\"^dgs?10$|^10[-_\\s]?year|^10y|^ust10\"],\n","    \"SP500_PE\":  [r\"^sp500[_\\s]?pe$|^pe$|^p\\/?e$\"],\n","    \"DJIA\":      [r\"^djia$|^dow|^dow[_\\s]?jones|^djia[_\\s]?close$\"],\n","}\n","DATE_HINTS = [r\"^date$\", r\"^month$\", r\"^time$\", r\"^period$\"]\n","\n","# =========================\n","# Helpers (no imputation)\n","# =========================\n","def _match_column(name, patterns):\n","    n = name.strip().lower()\n","    for pat in patterns:\n","        if re.search(pat, n, flags=re.I):\n","            return True\n","    return False\n","\n","def _find_date_col(df):\n","    # Try named hints\n","    for c in df.columns:\n","        if _match_column(str(c), DATE_HINTS):\n","            try:\n","                _ = pd.to_datetime(df[c], errors=\"raise\")\n","                return c\n","            except Exception:\n","                pass\n","    # Fallback: first datetime-like\n","    for c in df.columns:\n","        if pd.api.types.is_datetime64_any_dtype(df[c]):\n","            return c\n","    return None\n","\n","def _normalize_columns(df):\n","    # map canonical indicator -> column name (or None)\n","    mapping = {}\n","    cols = list(df.columns)\n","    for key, pats in INDICATOR_ALIASES.items():\n","        found = None\n","        for c in cols:\n","            if pd.api.types.is_numeric_dtype(df[c]) and _match_column(str(c), pats):\n","                found = c; break\n","        mapping[key] = found\n","    return mapping\n","\n","def _levels_24(df, colname):\n","    # Return a float array of length 24 with NaNs preserved (no imputation)\n","    s = pd.to_numeric(df[colname], errors=\"coerce\").astype(float)\n","    vals = s.to_numpy()\n","    if len(vals) < WINDOW_LEN:\n","        raise ValueError(f\"{colname}: only {len(vals)} rows available (need >= {WINDOW_LEN}).\")\n","    return vals[-WINDOW_LEN:] if TAKE_LAST_24 else vals[:WINDOW_LEN]\n","\n","def _pct_changes_with_nans(levels):\n","    # r_t = 100 * (x_t - x_{t-1}) / x_{t-1}; if either side is NaN or x_{t-1}==0 → NaN\n","    x = np.asarray(levels, dtype=float)\n","    r = np.full(WINDOW_LEN-1, np.nan, dtype=float)\n","    for t in range(1, len(x)):\n","        a, b = x[t-1], x[t]\n","        if np.isfinite(a) and np.isfinite(b) and (a != 0.0):\n","            r[t-1] = 100.0 * (b - a) / a\n","    return r  # length 23, may contain NaNs\n","\n","def _ols_slope_and_r2_with_missing(y):\n","    # Use only valid points; regress y on original time indices where valid.\n","    idx = np.where(np.isfinite(y))[0]\n","    if len(idx) < 3:\n","        return np.nan, np.nan\n","    t = (idx + 1).astype(float)  # 1..23 on valid slots\n","    yv = y[idx].astype(float)\n","    t_mean, y_mean = t.mean(), yv.mean()\n","    cov = np.sum((t - t_mean) * (yv - y_mean))\n","    var_t = np.sum((t - t_mean) ** 2)\n","    beta = cov / var_t if var_t != 0 else np.nan\n","    alpha = y_mean - beta * t_mean if np.isfinite(beta) else np.nan\n","    if not np.isfinite(beta) or not np.isfinite(alpha):\n","        return np.nan, np.nan\n","    y_hat = alpha + beta * t\n","    ssr = np.sum((yv - y_hat) ** 2)\n","    sst = np.sum((yv - y_mean) ** 2)\n","    r2 = np.nan if sst == 0 else (1.0 - ssr / sst)\n","    return float(beta), float(r2)\n","\n","def _std_sample_valid(a):\n","    v = a[np.isfinite(a)]\n","    return float(np.std(v, ddof=1)) if len(v) >= 2 else np.nan\n","\n","def _share_up(a):\n","    v = a[np.isfinite(a)]\n","    if len(v) == 0:\n","        return np.nan\n","    return float(np.mean(v > 0.0))\n","\n","def _std_subset(a, start_idx, end_idx):\n","    # a indices inclusive of [start_idx, end_idx]; both 0-based on returns array (length 23)\n","    sub = a[start_idx:end_idx+1]\n","    return _std_sample_valid(sub)\n","\n","def _contiguous_runs_valid(levels):\n","    x = np.asarray(levels, dtype=float)\n","    runs = []\n","    start = None\n","    for i, v in enumerate(x):\n","        if np.isfinite(v):\n","            if start is None:\n","                start = i\n","        else:\n","            if start is not None:\n","                runs.append((start, i-1))\n","                start = None\n","    if start is not None:\n","        runs.append((start, len(x)-1))\n","    return runs\n","\n","def _max_drawup_on_runs(levels):\n","    x = np.asarray(levels, dtype=float)\n","    best_up, best_peak = -np.inf, None\n","    for s, e in _contiguous_runs_valid(x):\n","        if e - s + 1 < 2:\n","            continue\n","        min_so_far = x[s]; t_peak_local = s\n","        up_local_best = 0.0\n","        for j in range(s+1, e+1):\n","            if x[j] / min_so_far - 1.0 > up_local_best:\n","                up_local_best = x[j] / min_so_far - 1.0\n","                t_peak_local = j\n","            if x[j] < min_so_far:\n","                min_so_far = x[j]\n","        if up_local_best > best_up:\n","            best_up = up_local_best\n","            best_peak = t_peak_local\n","    if best_peak is None:\n","        return np.nan, np.nan\n","    return 100.0 * best_up, int(best_peak + 1)  # 1-based index\n","\n","def _max_drawdown_on_runs(levels):\n","    x = np.asarray(levels, dtype=float)\n","    best_down, best_trough = +np.inf, None  # most negative %\n","    for s, e in _contiguous_runs_valid(x):\n","        if e - s + 1 < 2:\n","            continue\n","        max_so_far = x[s]; t_trough_local = s\n","        down_local_best = 0.0\n","        for j in range(s+1, e+1):\n","            d = (x[j] / max_so_far) - 1.0\n","            if d < down_local_best:\n","                down_local_best = d\n","                t_trough_local = j\n","            if x[j] > max_so_far:\n","                max_so_far = x[j]\n","        if down_local_best < best_down:\n","            best_down = down_local_best\n","            best_trough = t_trough_local\n","    if best_trough is None:\n","        return np.nan, np.nan\n","    return 100.0 * best_down, int(best_trough + 1)\n","\n","def _acf1_with_missing(y):\n","    if len(y) < 2:\n","        return np.nan\n","    y0 = y[:-1]; y1 = y[1:]\n","    mask = np.isfinite(y0) & np.isfinite(y1)\n","    if mask.sum() < 2:\n","        return np.nan\n","    a = y0[mask] - y0[mask].mean()\n","    b = y1[mask] - y1[mask].mean()\n","    denom = math.sqrt(np.sum(a*a) * np.sum(b*b))\n","    return float(0.0 if denom == 0 else np.sum(a*b) / denom)\n","\n","def _fmt_pct_or_uncertain(x, nd=1):\n","    return f\"{x:+.{nd}f}\" if np.isfinite(x) else \"uncertain\"\n","\n","def _fmt_share_or_uncertain(x, nd=2):\n","    return f\"{x:.{nd}f}\" if np.isfinite(x) else \"uncertain\"\n","\n","def summarize_indicator_levels_no_impute(levels):\n","    x = np.asarray(levels, dtype=float)\n","    if len(x) != WINDOW_LEN:\n","        raise ValueError(f\"Expected {WINDOW_LEN} monthly levels; got {len(x)}.\")\n","    r = _pct_changes_with_nans(x)\n","    net_change = np.nan\n","    if np.isfinite(x[0]) and np.isfinite(x[-1]) and x[0] != 0.0:\n","        net_change = 100.0 * (x[-1]/x[0] - 1.0)\n","    slope, r2 = _ols_slope_and_r2_with_missing(r)\n","    up_share  = _share_up(r)\n","    vol_std   = _std_sample_valid(r)\n","    std_early = _std_subset(r, 1, 7)   # r_2..r_8 -> indices 1..7\n","    std_late  = _std_subset(r, 17, 23) # r_18..r_24 -> 17..23\n","    vol_late_minus_early = (std_late - std_early) if (np.isfinite(std_early) and np.isfinite(std_late)) else np.nan\n","    max_up,  t_peak   = _max_drawup_on_runs(x)\n","    max_down, t_trough = _max_drawdown_on_runs(x)\n","    acf1 = _acf1_with_missing(r)\n","    return {\n","        \"net_change_24m_pct\":       _fmt_pct_or_uncertain(net_change, 1),\n","        \"slope_ols_pct_per_mo\":     _fmt_pct_or_uncertain(slope, 2),\n","        \"trend_r2\":                 _fmt_share_or_uncertain(r2, 2),\n","        \"up_month_share\":           _fmt_share_or_uncertain(up_share, 2),\n","        \"vol_std_pct\":              _fmt_pct_or_uncertain(vol_std, 1),\n","        \"vol_late_minus_early_pct\": _fmt_pct_or_uncertain(vol_late_minus_early, 1),\n","        \"max_drawup_pct\":           _fmt_pct_or_uncertain(max_up, 1),\n","        \"t_peak\":                   int(t_peak) if np.isfinite(t_peak) else \"uncertain\",\n","        \"max_drawdown_pct\":         _fmt_pct_or_uncertain(max_down, 1),\n","        \"t_trough\":                 int(t_trough) if np.isfinite(t_trough) else \"uncertain\",\n","        \"acf1\":                     _fmt_share_or_uncertain(acf1, 2),\n","    }\n","\n","# =========================\n","# Main: upload one CSV, process (no impute), and download SANITIZED YAML\n","# =========================\n","print(\"Upload ONE merged CSV (e.g., Merged_*_Last_24_months.csv)…\")\n","uploaded = _files.upload()\n","if not uploaded:\n","    raise SystemExit(\"No file uploaded.\")\n","\n","# Take the first uploaded file\n","in_name = list(uploaded.keys())[0]\n","raw = uploaded[in_name]\n","df = pd.read_csv(io.BytesIO(raw))\n","\n","# Sort by date if present (for windowing only; we do NOT emit dates)\n","dcol = _find_date_col(df)\n","if dcol is not None:\n","    df = df.copy()\n","    df[dcol] = pd.to_datetime(df[dcol], errors=\"coerce\")\n","    df = df.dropna(subset=[dcol]).sort_values(dcol, ascending=True, kind=\"mergesort\").reset_index(drop=True)\n","else:\n","    df = df.reset_index(drop=True)\n","\n","# Restrict deterministically to a 24-row window\n","if len(df) < WINDOW_LEN:\n","    raise ValueError(f\"File has only {len(df)} rows; need at least {WINDOW_LEN}.\")\n","df_24 = df.tail(WINDOW_LEN) if TAKE_LAST_24 else df.head(WINDOW_LEN)\n","\n","# Detect indicators present\n","mapping = _normalize_columns(df_24)\n","present = [k for k,v in mapping.items() if v is not None]\n","if not present:\n","    raise ValueError(\"No recognizable indicator columns found (CPI, PPI, FEDFUNDS, DGS10, SP500_PE, DJIA).\")\n","\n","# Build NL summaries (no imputation) and track missing months (indices only)\n","results = {}\n","missing_map = {}\n","for key in present:\n","    col = mapping[key]\n","    levels = _levels_24(df_24, col)  # preserves NaNs\n","    results[key] = summarize_indicator_levels_no_impute(levels)\n","    miss_idx = [i+1 for i,v in enumerate(levels) if not np.isfinite(v)]\n","    if miss_idx:\n","        missing_map[key] = miss_idx\n","\n","# Compose SANITIZED JSON (no filename, no dates)\n","anon_id = f\"anon_{uuid.uuid4().hex[:8]}\"\n","out_json = f\"{anon_id}_window_llm_noimpute.json\"\n","\n","output = {\n","    \"window\": \"24_months\",\n","    \"indicators\": results\n","}\n","if missing_map:\n","    output[\"missing_data\"] = missing_map\n","\n","with open(out_json, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(output, f, indent=2)\n","\n","print(f\"\\nIndicators detected: {present}\")\n","if missing_map:\n","    print(\"Missing months (1..24) by indicator:\", missing_map)\n","print(f\"Created JSON (sanitized): {out_json}\")\n","\n","# Auto-download to your local laptop\n","_files.download(out_json)"]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","# Colab: Upload ONE merged CSV (>=2 rows) → produce ONE SANITIZED LLM-ready JSON (no imputation) and download it.\n","# Sanitization: no original filename, no calendar dates/timeline in the JSON.\n","# This version processes the full dataset, not a fixed-size window.\n","\n","import io, os, re, math, json, uuid\n","import numpy as np\n","import pandas as pd\n","\n","# ---- Colab upload helper\n","try:\n","    from google.colab import files as _files\n","except Exception as e:\n","    raise RuntimeError(\"This cell is intended to run in Google Colab.\") from e\n","\n","# =========================\n","# Config\n","# =========================\n","# Canonical indicator keys and flexible aliases (case-insensitive)\n","INDICATOR_ALIASES = {\n","    \"CPI\":       [r\"^cpi$\"],\n","    \"PPI\":       [r\"^ppi$\"],\n","    \"FEDFUNDS\":  [r\"^fedfunds$|^federal[_\\s]?funds|^ffr$|^policy[_\\s]?rate$\"],\n","    \"DGS10\":     [r\"^dgs?10$|^10[-_\\s]?year|^10y|^ust10\"],\n","    \"SP500_PE\":  [r\"^sp500[_\\s]?pe$|^pe$|^p\\/?e$\"],\n","    \"DJIA\":      [r\"^djia$|^dow|^dow[_\\s]?jones|^djia[_\\s]?close$\"],\n","}\n","DATE_HINTS = [r\"^date$\", r\"^month$\", r\"^time$\", r\"^period$\"]\n","\n","# =========================\n","# Helpers (no imputation)\n","# =========================\n","def _match_column(name, patterns):\n","    n = name.strip().lower()\n","    for pat in patterns:\n","        if re.search(pat, n, flags=re.I):\n","            return True\n","    return False\n","\n","def _find_date_col(df):\n","    # Try named hints\n","    for c in df.columns:\n","        if _match_column(str(c), DATE_HINTS):\n","            try:\n","                _ = pd.to_datetime(df[c], errors=\"raise\")\n","                return c\n","            except Exception:\n","                pass\n","    # Fallback: first datetime-like\n","    for c in df.columns:\n","        if pd.api.types.is_datetime64_any_dtype(df[c]):\n","            return c\n","    return None\n","\n","def _normalize_columns(df):\n","    # map canonical indicator -> column name (or None)\n","    mapping = {}\n","    cols = list(df.columns)\n","    for key, pats in INDICATOR_ALIASES.items():\n","        found = None\n","        for c in cols:\n","            if pd.api.types.is_numeric_dtype(df[c]) and _match_column(str(c), pats):\n","                found = c; break\n","        mapping[key] = found\n","    return mapping\n","\n","def _levels_all(df, colname):\n","    # Return a float array with NaNs preserved (no imputation), using ALL available rows\n","    s = pd.to_numeric(df[colname], errors=\"coerce\").astype(float)\n","    vals = s.to_numpy()\n","    if len(vals) < 2:\n","        raise ValueError(f\"{colname}: need at least 2 rows to compute returns; got {len(vals)}.\")\n","    return vals\n","\n","def _pct_changes_with_nans(levels):\n","    # r_t = 100 * (x_t - x_{t-1}) / x_{t-1}; if either side is NaN or x_{t-1}==0 → NaN\n","    x = np.asarray(levels, dtype=float)\n","    R = max(0, len(x) - 1)\n","    r = np.full(R, np.nan, dtype=float)\n","    for t in range(1, len(x)):\n","        a, b = x[t-1], x[t]\n","        if np.isfinite(a) and np.isfinite(b) and (a != 0.0):\n","            r[t-1] = 100.0 * (b - a) / a\n","    return r  # length = len(levels) - 1, may contain NaNs\n","\n","def _ols_slope_and_r2_with_missing(y):\n","    # Use only valid points; regress y on original time indices where valid.\n","    idx = np.where(np.isfinite(y))[0]\n","    if len(idx) < 3:\n","        return np.nan, np.nan\n","    t = (idx + 1).astype(float)  # 1..R on valid slots\n","    yv = y[idx].astype(float)\n","    t_mean, y_mean = t.mean(), yv.mean()\n","    cov = np.sum((t - t_mean) * (yv - y_mean))\n","    var_t = np.sum((t - t_mean) ** 2)\n","    beta = cov / var_t if var_t != 0 else np.nan\n","    alpha = y_mean - beta * t_mean if np.isfinite(beta) else np.nan\n","    if not np.isfinite(beta) or not np.isfinite(alpha):\n","        return np.nan, np.nan\n","    y_hat = alpha + beta * t\n","    ssr = np.sum((yv - y_hat) ** 2)\n","    sst = np.sum((yv - y_mean) ** 2)\n","    r2 = np.nan if sst == 0 else (1.0 - ssr / sst)\n","    return float(beta), float(r2)\n","\n","def _std_sample_valid(a):\n","    v = a[np.isfinite(a)]\n","    return float(np.std(v, ddof=1)) if len(v) >= 2 else np.nan\n","\n","def _share_up(a):\n","    v = a[np.isfinite(a)]\n","    if len(v) == 0:\n","        return np.nan\n","    return float(np.mean(v > 0.0))\n","\n","def _contiguous_runs_valid(levels):\n","    x = np.asarray(levels, dtype=float)\n","    runs = []\n","    start = None\n","    for i, v in enumerate(x):\n","        if np.isfinite(v):\n","            if start is None:\n","                start = i\n","        else:\n","            if start is not None:\n","                runs.append((start, i-1))\n","                start = None\n","    if start is not None:\n","        runs.append((start, len(x)-1))\n","    return runs\n","\n","def _max_drawup_on_runs(levels):\n","    x = np.asarray(levels, dtype=float)\n","    best_up, best_peak = -np.inf, None\n","    for s, e in _contiguous_runs_valid(x):\n","        if e - s + 1 < 2:\n","            continue\n","        min_so_far = x[s]; t_peak_local = s\n","        up_local_best = 0.0\n","        for j in range(s+1, e+1):\n","            if x[j] / min_so_far - 1.0 > up_local_best:\n","                up_local_best = x[j] / min_so_far - 1.0\n","                t_peak_local = j\n","            if x[j] < min_so_far:\n","                min_so_far = x[j]\n","        if up_local_best > best_up:\n","            best_up = up_local_best\n","            best_peak = t_peak_local\n","    if best_peak is None:\n","        return np.nan, np.nan\n","    return 100.0 * best_up, int(best_peak + 1)  # 1-based index\n","\n","def _max_drawdown_on_runs(levels):\n","    x = np.asarray(levels, dtype=float)\n","    best_down, best_trough = +np.inf, None  # most negative %\n","    for s, e in _contiguous_runs_valid(x):\n","        if e - s + 1 < 2:\n","            continue\n","        max_so_far = x[s]; t_trough_local = s\n","        down_local_best = 0.0\n","        for j in range(s+1, e+1):\n","            d = (x[j] / max_so_far) - 1.0\n","            if d < down_local_best:\n","                down_local_best = d\n","                t_trough_local = j\n","            if x[j] > max_so_far:\n","                max_so_far = x[j]\n","        if down_local_best < best_down:\n","            best_down = down_local_best\n","            best_trough = t_trough_local\n","    if best_trough is None:\n","        return np.nan, np.nan\n","    return 100.0 * best_down, int(best_trough + 1)\n","\n","def _acf1_with_missing(y):\n","    if len(y) < 2:\n","        return np.nan\n","    y0 = y[:-1]; y1 = y[1:]\n","    mask = np.isfinite(y0) & np.isfinite(y1)\n","    if mask.sum() < 2:\n","        return np.nan\n","    a = y0[mask] - y0[mask].mean()\n","    b = y1[mask] - y1[mask].mean()\n","    denom = math.sqrt(np.sum(a*a) * np.sum(b*b))\n","    return float(0.0 if denom == 0 else np.sum(a*b) / denom)\n","\n","def _fmt_pct_or_uncertain(x, nd=1):\n","    return f\"{x:+.{nd}f}\" if np.isfinite(x) else \"uncertain\"\n","\n","def _fmt_share_or_uncertain(x, nd=2):\n","    return f\"{x:.{nd}f}\" if np.isfinite(x) else \"uncertain\"\n","\n","def _std_early_late_general(r):\n","    \"\"\"\n","    Compute early/late volatility on a percentage basis.\n","    The window size for early/late volatility is scaled based on the total\n","    length of the data series. This ensures a consistent comparison even\n","    with different dataset sizes.\n","    Returns (std_early, std_late) with NaN → 'uncertain'.\n","    \"\"\"\n","    R = len(r)\n","    if R < 2:\n","        return np.nan, np.nan\n","    if R >= 23:\n","        early_start, early_end = 1, 7\n","        late_start,  late_end  = R - 7, R - 1\n","    else:\n","        k = max(2, int(round(7 * R / 23.0)))\n","        early_start = 1 if R > 1 else 0\n","        early_end   = min(R - 1, early_start + k - 1)\n","        late_start  = max(0, R - k)\n","        late_end    = R - 1\n","    std_early = _std_sample_valid(r[early_start:early_end+1])\n","    std_late  = _std_sample_valid(r[late_start:late_end+1])\n","    return std_early, std_late\n","\n","def summarize_indicator_levels_no_impute(levels):\n","    x = np.asarray(levels, dtype=float)\n","    N = len(x)\n","    if N < 2:\n","        raise ValueError(f\"Expected at least 2 rows; got {N}.\")\n","    r = _pct_changes_with_nans(x)\n","    # Net change over the whole available window\n","    net_change = np.nan\n","    if np.isfinite(x[0]) and np.isfinite(x[-1]) and x[0] != 0.0:\n","        net_change = 100.0 * (x[-1]/x[0] - 1.0)\n","    slope, r2 = _ols_slope_and_r2_with_missing(r)\n","    up_share  = _share_up(r)\n","    vol_std   = _std_sample_valid(r)\n","    std_early, std_late = _std_early_late_general(r)\n","    vol_late_minus_early = (std_late - std_early) if (np.isfinite(std_early) and np.isfinite(std_late)) else np.nan\n","    max_up,  t_peak   = _max_drawup_on_runs(x)\n","    max_down, t_trough = _max_drawdown_on_runs(x)\n","    acf1 = _acf1_with_missing(r)\n","    return {\n","        # Keep original field names & \"uncertain\" formatting\n","        \"net_change_pct\":           _fmt_pct_or_uncertain(net_change, 1),\n","        \"slope_ols_pct_per_mo\":     _fmt_pct_or_uncertain(slope, 2),\n","        \"trend_r2\":                 _fmt_share_or_uncertain(r2, 2),\n","        \"up_month_share\":           _fmt_share_or_uncertain(up_share, 2),\n","        \"vol_std_pct\":              _fmt_pct_or_uncertain(vol_std, 1),\n","        \"vol_late_minus_early_pct\": _fmt_pct_or_uncertain(vol_late_minus_early, 1),\n","        \"max_drawup_pct\":           _fmt_pct_or_uncertain(max_up, 1),\n","        \"t_peak\":                   int(t_peak) if np.isfinite(t_peak) else \"uncertain\",\n","        \"max_drawdown_pct\":         _fmt_pct_or_uncertain(max_down, 1),\n","        \"t_trough\":                 int(t_trough) if np.isfinite(t_trough) else \"uncertain\",\n","        \"acf1\":                     _fmt_share_or_uncertain(acf1, 2),\n","    }\n","\n","# =========================\n","# Main: upload one CSV, process (no impute), and download SANITIZED JSON\n","# =========================\n","print(\"Upload ONE merged CSV (e.g., Merged_*.csv)…\")\n","uploaded = _files.upload()\n","if not uploaded:\n","    raise SystemExit(\"No file uploaded.\")\n","\n","# Take the first uploaded file\n","in_name = list(uploaded.keys())[0]\n","raw = uploaded[in_name]\n","df = pd.read_csv(io.BytesIO(raw))\n","\n","# Sort by date if present (for ordering only; we do NOT emit dates)\n","dcol = _find_date_col(df)\n","if dcol is not None:\n","    df = df.copy()\n","    df[dcol] = pd.to_datetime(df[dcol], errors=\"coerce\")\n","    df = df.dropna(subset=[dcol]).sort_values(dcol, ascending=True, kind=\"mergesort\").reset_index(drop=True)\n","else:\n","    df = df.reset_index(drop=True)\n","\n","# Require at least 2 rows overall (no fixed 24-row slice)\n","if len(df) < 2:\n","    raise ValueError(f\"File has only {len(df)} rows; need at least 2.\")\n","\n","# Detect indicators present over ALL rows\n","mapping = _normalize_columns(df)\n","present = [k for k,v in mapping.items() if v is not None]\n","if not present:\n","    raise ValueError(\"No recognizable indicator columns found (CPI, PPI, FEDFUNDS, DGS10, SP500_PE, DJIA).\")\n","\n","# Build NL summaries (no imputation) and track missing indices (1-based)\n","results = {}\n","missing_map = {}\n","for key in present:\n","    col = mapping[key]\n","    levels = _levels_all(df, col)  # preserves NaNs across ALL rows\n","    results[key] = summarize_indicator_levels_no_impute(levels)\n","    miss_idx = [i+1 for i,v in enumerate(levels) if not np.isfinite(v)]\n","    if miss_idx:\n","        missing_map[key] = miss_idx\n","\n","# Compose SANITIZED JSON (no filename, no dates)\n","anon_id = f\"anon_{uuid.uuid4().hex[:8]}\"\n","out_json = f\"{anon_id}_window_llm_noimpute.json\"\n","\n","output = {\n","    # Keep the same key name but clarify it's the whole series\n","    \"window\": \"full_series_rows\",\n","    \"indicators\": results\n","}\n","if missing_map:\n","    output[\"missing_data\"] = missing_map  # same key name as original\n","\n","with open(out_json, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(output, f, indent=2)\n","\n","print(f\"\\nIndicators detected: {present}\")\n","if missing_map:\n","    print(\"Missing months (1..N) by indicator:\", missing_map)\n","print(f\"Created JSON (sanitized): {out_json}\")\n","\n","# Auto-download to your local laptop\n","_files.download(out_json)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"id":"DdcdXET4Csei","executionInfo":{"status":"ok","timestamp":1756535687381,"user_tz":-540,"elapsed":3618,"user":{"displayName":"BumBum Cho","userId":"04453444281969177716"}},"outputId":"88683bf1-a92d-4a9e-cc76-964ec071caaf"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Upload ONE merged CSV (e.g., Merged_*.csv)…\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-df0af3bd-1c24-4c20-b8de-bd0fe848870d\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-df0af3bd-1c24-4c20-b8de-bd0fe848870d\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving 2001.04-2003.12.csv to 2001.04-2003.12.csv\n","\n","Indicators detected: ['CPI', 'PPI', 'FEDFUNDS', 'DGS10', 'SP500_PE', 'DJIA']\n","Created JSON (sanitized): anon_d6008fd9_window_llm_noimpute.json\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_982e4226-9b99-4045-acdf-c6e7d2da3f12\", \"anon_d6008fd9_window_llm_noimpute.json\", 2173)"]},"metadata":{}}]}]}